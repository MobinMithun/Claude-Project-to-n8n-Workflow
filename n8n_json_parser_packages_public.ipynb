{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gJk_1zU5Uaem",
        "v3LZsh63UfU4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MobinMithun/Claude-Project-to-n8n-Workflow/blob/main/n8n_json_parser_packages_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse the Unique JSONs"
      ],
      "metadata": {
        "id": "gJk_1zU5Uaem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import hashlib\n",
        "\n",
        "# For running in Colab and mounting Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MOUNT GOOGLE DRIVE\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. CONFIGURE PATHS\n",
        "# --------------------------------------------------------------------\n",
        "source_folder_path = \"/content/drive/MyDrive/n8n Workflows\"\n",
        "output_folder_path = \"/content/drive/MyDrive/n8n_Workflow_Extracts\"\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "mega_text_path = os.path.join(output_folder_path, \"ALL_unique_nodes.txt\")\n",
        "mega_json_path = os.path.join(output_folder_path, \"ALL_unique_nodes.json\")\n",
        "\n",
        "all_files = glob.glob(os.path.join(source_folder_path, \"*\"))\n",
        "\n",
        "print(\"=====================================================\")\n",
        "print(\"Looking for workflow files in:\", source_folder_path)\n",
        "print(\"=====================================================\\n\")\n",
        "time.sleep(1)\n",
        "\n",
        "if not all_files:\n",
        "    print(\"No files found at all! Double-check your folder path.\")\n",
        "else:\n",
        "    print(f\"Found {len(all_files)} files total.\\n\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. HOW WE DEFINE \"UNIQUENESS\"\n",
        "# --------------------------------------------------------------------\n",
        "def node_signature(node):\n",
        "    \"\"\"\n",
        "    Hash the entire node JSON, ensuring that any difference\n",
        "    in structure or parameters yields a separate entry.\n",
        "    \"\"\"\n",
        "    node_str = json.dumps(node, sort_keys=True)\n",
        "    return hashlib.md5(node_str.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 4. DATA STRUCTURES\n",
        "# --------------------------------------------------------------------\n",
        "found_nodes = set()    # set of node signatures\n",
        "unique_node_data = []  # list of all unique node dicts\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 5. PARSE EACH FILE & EXTRACT UNIQUE NODES\n",
        "# --------------------------------------------------------------------\n",
        "valid_file_count = 0\n",
        "for file_path in all_files:\n",
        "    extension = os.path.splitext(file_path)[1].lower()\n",
        "    if extension not in [\".json\", \".txt\"]:\n",
        "        print(f\"Skipping (not .json or .txt): {os.path.basename(file_path)}\")\n",
        "        continue\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    print(f\"Attempting to parse '{file_name}'...\")\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            raw_content = f.read().strip()\n",
        "            data = json.loads(raw_content)  # parse as JSON\n",
        "    except Exception as e:\n",
        "        print(f\"  -> Skipping file (parse error): {file_name}\\n     Error: {e}\")\n",
        "        continue\n",
        "\n",
        "    valid_file_count += 1\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        print(f\"  -> Skipping (parsed JSON not a dict): {file_name}\")\n",
        "        continue\n",
        "\n",
        "    nodes = data.get(\"nodes\", [])\n",
        "    if not nodes:\n",
        "        print(f\"  -> Skipping (no 'nodes' key or empty) in: {file_name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  -> '{file_name}' has {len(nodes)} node(s). Extracting unique signatures...\")\n",
        "\n",
        "    for node in nodes:\n",
        "        sig = node_signature(node)\n",
        "        if sig not in found_nodes:\n",
        "            found_nodes.add(sig)\n",
        "            unique_node_data.append(node)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 6. WRITE OUT THE UNIQUE NODES (FULL REFERENCE)\n",
        "# --------------------------------------------------------------------\n",
        "if not unique_node_data:\n",
        "    print(\"\\nNo unique nodes were found.\")\n",
        "else:\n",
        "    print(f\"\\nTotal unique node definitions found: {len(unique_node_data)}\\n\")\n",
        "\n",
        "    # 6A. Big text file\n",
        "    with open(mega_text_path, \"w\", encoding=\"utf-8\") as mega_txt:\n",
        "        for i, node in enumerate(unique_node_data, 1):\n",
        "            mega_txt.write(f\"================ Node #{i} ================\\n\")\n",
        "            mega_txt.write(json.dumps(node, indent=2))\n",
        "            mega_txt.write(\"\\n\\n\")\n",
        "\n",
        "    # 6B. Big JSON array\n",
        "    with open(mega_json_path, \"w\", encoding=\"utf-8\") as mega_json:\n",
        "        json.dump(unique_node_data, mega_json, indent=2)\n",
        "\n",
        "    print(\"Wrote all unique nodes to:\")\n",
        "    print(f\" - {mega_text_path}\")\n",
        "    print(f\" - {mega_json_path}\")\n",
        "\n",
        "print(\"=====================================================\")\n",
        "print(f\"Processing complete!\")\n",
        "print(f\" - Valid text/JSON files processed: {valid_file_count}\")\n",
        "print(f\" - Unique node definitions found: {len(unique_node_data)}\")\n",
        "print(f\" - Output folder: {output_folder_path}\")\n",
        "print(\"=====================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OagokiYCHz7C",
        "outputId": "e0c6950e-3a4c-4305-e830-34f3618e557b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "=====================================================\n",
            "Looking for workflow files in: /content/drive/MyDrive/n8n Workflows\n",
            "=====================================================\n",
            "\n",
            "Found 294 files total.\n",
            "\n",
            "Attempting to parse '🤖🧠 AI Agent Chatbot + LONG TERM Memory + Note Storage + Telegram.txt'...\n",
            "  -> '🤖🧠 AI Agent Chatbot + LONG TERM Memory + Note Storage + Telegram.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Open Deep Research - AI-Powered Autonomous Research Workflow.txt'...\n",
            "  -> 'Open Deep Research - AI-Powered Autonomous Research Workflow.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🐋🤖 DeepSeek AI Agent + Telegram + LONG TERM Memory 🧠.txt'...\n",
            "  -> '🐋🤖 DeepSeek AI Agent + Telegram + LONG TERM Memory 🧠.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with Postgresql Database.txt'...\n",
            "  -> 'Chat with Postgresql Database.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Upload to Instagram and Tiktok from Google Drive.txt'...\n",
            "  -> 'Upload to Instagram and Tiktok from Google Drive.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Automated HR Workflow for CV Analysis and Candidate Evaluation.txt'...\n",
            "  -> 'AI Automated HR Workflow for CV Analysis and Candidate Evaluation.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants.txt'...\n",
            "  -> 'AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Content Generator for WordPress with DeepSeek R1.txt'...\n",
            "  -> 'Automate Content Generator for WordPress with DeepSeek R1.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'RAG_Context-Aware Chunking _ Google Drive to Pinecone via OpenRouter & Gemini.txt'...\n",
            "  -> 'RAG_Context-Aware Chunking _ Google Drive to Pinecone via OpenRouter & Gemini.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Effortless Email Management with AI-Powered Summarization & Review.txt'...\n",
            "  -> 'Effortless Email Management with AI-Powered Summarization & Review.txt' has 31 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate Instagram Content from Top Trends with AI Image Generation.txt'...\n",
            "  -> 'Generate Instagram Content from Top Trends with AI Image Generation.txt' has 44 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI.txt'...\n",
            "  -> 'Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'A Very Simple _Human in the Loop_ Email Response System Using AI and IMAP.txt'...\n",
            "  -> 'A Very Simple _Human in the Loop_ Email Response System Using AI and IMAP.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Create a Branded AI-Powered Website Chatbot.txt'...\n",
            "  -> 'Create a Branded AI-Powered Website Chatbot.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🐋DeepSeek V3 Chat & R1 Reasoning Quick Start.txt'...\n",
            "  -> '🐋DeepSeek V3 Chat & R1 Reasoning Quick Start.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Slack slash commands AI Chat Bot.txt'...\n",
            "  -> 'Slack slash commands AI Chat Bot.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🤖 Telegram Messaging Agent for Text_Audio_Images.txt'...\n",
            "  -> '🤖 Telegram Messaging Agent for Text_Audio_Images.txt' has 39 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'RAG Chatbot for Company Documents using Google Drive and Gemini.txt'...\n",
            "  -> 'RAG Chatbot for Company Documents using Google Drive and Gemini.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-powered email processing autoresponder and response approval (Yes_No).txt'...\n",
            "  -> 'AI-powered email processing autoresponder and response approval (Yes_No).txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Social Media Analysis and Automated Email Generation.txt'...\n",
            "  -> 'Social Media Analysis and Automated Email Generation.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🔐🦙🤖 Private & Local Ollama Self-Hosted AI Assistant.txt'...\n",
            "  -> '🔐🦙🤖 Private & Local Ollama Self-Hosted AI Assistant.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Email Automation for Business_ Summarize & Respond with RAG.txt'...\n",
            "  -> 'AI-Powered Email Automation for Business_ Summarize & Respond with RAG.txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Fetch Dynamic Prompts from GitHub and Auto-Populate n8n Expressions in Prompt.txt'...\n",
            "  -> 'Fetch Dynamic Prompts from GitHub and Auto-Populate n8n Expressions in Prompt.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Use OpenRouter in n8n versions _1.78.txt'...\n",
            "  -> 'Use OpenRouter in n8n versions _1.78.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse '⚡AI-Powered YouTube Video Summarization & Analysis.txt'...\n",
            "  -> '⚡AI-Powered YouTube Video Summarization & Analysis.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3.txt'...\n",
            "  -> Skipping file (parse error): Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3.txt\n",
            "     Error: Extra data: line 2942 column 2 (char 72213)\n",
            "Attempting to parse 'Query Perplexity AI from your n8n workflows.txt'...\n",
            "  -> 'Query Perplexity AI from your n8n workflows.txt' has 6 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Email Summary Agent.txt'...\n",
            "  -> 'Email Summary Agent.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API.txt'...\n",
            "  -> 'Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Scrape Trustpilot Reviews with DeepSeek, Analyze Sentiment with OpenAI.txt'...\n",
            "  -> 'Scrape Trustpilot Reviews with DeepSeek, Analyze Sentiment with OpenAI.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Simple Expense Tracker with n8n Chat, AI Agent and Google Sheets.txt'...\n",
            "  -> 'Simple Expense Tracker with n8n Chat, AI Agent and Google Sheets.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'HR & IT Helpdesk Chatbot with Audio Transcription.txt'...\n",
            "  -> 'HR & IT Helpdesk Chatbot with Audio Transcription.txt' has 27 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent _ Google calendar assistant using OpenAI.txt'...\n",
            "  -> 'AI Agent _ Google calendar assistant using OpenAI.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract license plate number from image uploaded via an n8n form.txt'...\n",
            "  -> 'Extract license plate number from image uploaded via an n8n form.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🔍 Perplexity Research to HTML_ AI-Powered Content Creation.txt'...\n",
            "  -> Skipping file (parse error): 🔍 Perplexity Research to HTML_ AI-Powered Content Creation.txt\n",
            "     Error: Extra data: line 1389 column 2 (char 33965)\n",
            "Attempting to parse 'Modular & Customizable AI-Powered Email Routing_ Text Classifier for eCommerce.txt'...\n",
            "  -> 'Modular & Customizable AI-Powered Email Routing_ Text Classifier for eCommerce.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack.txt'...\n",
            "  -> 'AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack.txt' has 31 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Proxmox AI Agent with n8n and Generative AI Integration.txt'...\n",
            "  -> 'Proxmox AI Agent with n8n and Generative AI Integration.txt' has 35 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate SIEM Alert Enrichment with MITRE ATT&CK, Qdrant & Zendesk in n8n.txt'...\n",
            "  -> 'Automate SIEM Alert Enrichment with MITRE ATT&CK, Qdrant & Zendesk in n8n.txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Personal Shopper Chatbot for WooCommerce with RAG using Google Drive and openAI.txt'...\n",
            "  -> 'Personal Shopper Chatbot for WooCommerce with RAG using Google Drive and openAI.txt' has 25 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with GitHub API Documentation_ RAG-Powered Chatbot with Pinecone & OpenAI.txt'...\n",
            "  -> 'Chat with GitHub API Documentation_ RAG-Powered Chatbot with Pinecone & OpenAI.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Building Your First WhatsApp Chatbot.txt'...\n",
            "  -> 'Building Your First WhatsApp Chatbot.txt' has 28 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Building Your First WhatsApp Chatbot (1).txt'...\n",
            "  -> 'Building Your First WhatsApp Chatbot (1).txt' has 28 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent To Chat With Files In Supabase Storage.txt'...\n",
            "  -> 'AI Agent To Chat With Files In Supabase Storage.txt' has 33 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Detect hallucinations using specialised Ollama model bespoke-minicheck.txt'...\n",
            "  -> 'Detect hallucinations using specialised Ollama model bespoke-minicheck.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Build an OpenAI Assistant with Google Drive Integration.txt'...\n",
            "  -> 'Build an OpenAI Assistant with Google Drive Integration.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered RAG Workflow For Stock Earnings Report Analysis.txt'...\n",
            "  -> 'AI-Powered RAG Workflow For Stock Earnings Report Analysis.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'HR Job Posting and Evaluation with AI.txt'...\n",
            "  -> 'HR Job Posting and Evaluation with AI.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Auto-Tag Blog Posts in WordPress with AI.txt'...\n",
            "  -> 'Auto-Tag Blog Posts in WordPress with AI.txt' has 32 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI agent for Instagram DM_inbox. Manychat + Open AI integration.txt'...\n",
            "  -> 'AI agent for Instagram DM_inbox. Manychat + Open AI integration.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract and process information directly from PDF using Claude and Gemini.txt'...\n",
            "  -> 'Extract and process information directly from PDF using Claude and Gemini.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Basic Automatic Gmail Email Labelling with OpenAI and Gmail API.txt'...\n",
            "  -> 'Basic Automatic Gmail Email Labelling with OpenAI and Gmail API.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Zoom AI Meeting Assistant creates mail summary, ClickUp tasks and follow-up call.txt'...\n",
            "  -> 'Zoom AI Meeting Assistant creates mail summary, ClickUp tasks and follow-up call.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Fitness Coach Strava Data Analysis and Personalized Training Insights.txt'...\n",
            "  -> 'AI Fitness Coach Strava Data Analysis and Personalized Training Insights.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Social Media Amplifier.txt'...\n",
            "  -> 'AI-Powered Social Media Amplifier.txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports.txt'...\n",
            "  -> 'UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyze tradingview.com charts with Chrome extension, N8N and OpenAI.txt'...\n",
            "  -> 'Analyze tradingview.com charts with Chrome extension, N8N and OpenAI.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'BambooHR AI-Powered Company Policies and Benefits Chatbot.txt'...\n",
            "  -> 'BambooHR AI-Powered Company Policies and Benefits Chatbot.txt' has 50 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Youtube Trend Finder Based On Niche.txt'...\n",
            "  -> 'AI Youtube Trend Finder Based On Niche.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Social Media Caption Creator creates social media post captions in Airtable.txt'...\n",
            "  -> 'AI Social Media Caption Creator creates social media post captions in Airtable.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🎨 Interactive Image Editor with FLUX.1 Fill Tool for Inpainting.txt'...\n",
            "  -> '🎨 Interactive Image Editor with FLUX.1 Fill Tool for Inpainting.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent to chat with Airtable and analyze data.txt'...\n",
            "  -> Skipping file (parse error): AI Agent to chat with Airtable and analyze data.txt\n",
            "     Error: Extra data: line 1397 column 2 (char 31889)\n",
            "Attempting to parse 'AI Data Extraction with Dynamic Prompts and Airtable.txt'...\n",
            "  -> 'AI Data Extraction with Dynamic Prompts and Airtable.txt' has 51 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract personal data with self-hosted LLM Mistral NeMo.txt'...\n",
            "  -> 'Extract personal data with self-hosted LLM Mistral NeMo.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize the New Documents from Google Drive and Save Summary in Google Sheet.txt'...\n",
            "  -> 'Summarize the New Documents from Google Drive and Save Summary in Google Sheet.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Agentic Telegram AI bot with with LangChain nodes and new tools.txt'...\n",
            "  -> 'Agentic Telegram AI bot with with LangChain nodes and new tools.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Generated Summary Block for WordPress Posts.txt'...\n",
            "  -> 'AI-Generated Summary Block for WordPress Posts.txt' has 32 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Microsoft Outlook AI Email Assistant with contact support from Monday and Airtable.txt'...\n",
            "  -> 'Microsoft Outlook AI Email Assistant with contact support from Monday and Airtable.txt' has 28 node(s). Extracting unique signatures...\n",
            "Attempting to parse '✨ Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini.txt'...\n",
            "  -> '✨ Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Angie, Personal AI Assistant with Telegram Voice and Text.txt'...\n",
            "  -> 'Angie, Personal AI Assistant with Telegram Voice and Text.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize YouTube Videos from Transcript.txt'...\n",
            "  -> 'Summarize YouTube Videos from Transcript.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Blog Creation in Brand Voice with AI.txt'...\n",
            "  -> 'Automate Blog Creation in Brand Voice with AI.txt' has 27 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Driven Lead Management and Inquiry Automation with ERPNext & n8n.txt'...\n",
            "  -> Skipping file (parse error): AI-Driven Lead Management and Inquiry Automation with ERPNext & n8n.txt\n",
            "     Error: Extra data: line 733 column 2 (char 20652)\n",
            "Attempting to parse 'API Schema Extractor.txt'...\n",
            "  -> 'API Schema Extractor.txt' has 88 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Powered Web Scraping with Jina, Google Sheets and OpenAI _ the EASY way.txt'...\n",
            "  -> 'AI Powered Web Scraping with Jina, Google Sheets and OpenAI _ the EASY way.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Respond to WhatsApp Messages with AI Like a Pro!.txt'...\n",
            "  -> 'Respond to WhatsApp Messages with AI Like a Pro!.txt' has 35 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Spot Workplace Discrimination Patterns with AI.txt'...\n",
            "  -> 'Spot Workplace Discrimination Patterns with AI.txt' has 38 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Make OpenAI Citation for File Retrieval RAG.txt'...\n",
            "  -> 'Make OpenAI Citation for File Retrieval RAG.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Daily meetings summarization with Gemini AI.txt'...\n",
            "  -> 'Daily meetings summarization with Gemini AI.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyse papers from Hugging Face with AI and store them in Notion.txt'...\n",
            "  -> 'Analyse papers from Hugging Face with AI and store them in Notion.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Create a Google Analytics Data Report with AI and sent it to E-Mail and Telegram.txt'...\n",
            "  -> 'Create a Google Analytics Data Report with AI and sent it to E-Mail and Telegram.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract text from PDF and image using Vertex AI (Gemini) into CSV.txt'...\n",
            "  -> 'Extract text from PDF and image using Vertex AI (Gemini) into CSV.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Hacker News to Video Content.txt'...\n",
            "  -> 'Hacker News to Video Content.txt' has 48 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram AI bot assistant_ ready-made template for voice & text messages.txt'...\n",
            "  -> 'Telegram AI bot assistant_ ready-made template for voice & text messages.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Notion to Pinecone Vector Store Integration.txt'...\n",
            "  -> 'Notion to Pinecone Vector Store Integration.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Conversational Interviews with AI Agents and n8n Forms.txt'...\n",
            "  -> 'Conversational Interviews with AI Agents and n8n Forms.txt' has 40 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent to chat with Supabase_PostgreSQL DB.txt'...\n",
            "  -> 'AI Agent to chat with Supabase_PostgreSQL DB.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automated End-to-End Fine-Tuning of OpenAI Models with Google Drive Integration.txt'...\n",
            "  -> 'Automated End-to-End Fine-Tuning of OpenAI Models with Google Drive Integration.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Hacker News Throwback Machine - See What Was Hot on This Day, Every Year!.txt'...\n",
            "  -> 'Hacker News Throwback Machine - See What Was Hot on This Day, Every Year!.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Flux AI Image Generator.txt'...\n",
            "  -> 'Flux AI Image Generator.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat Assistant (OpenAI assistant) with Postgres Memory And API Calling Capabalities.txt'...\n",
            "  -> 'Chat Assistant (OpenAI assistant) with Postgres Memory And API Calling Capabalities.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse '📚 Auto-generate documentation for n8n workflows with GPT and Docsify.txt'...\n",
            "  -> '📚 Auto-generate documentation for n8n workflows with GPT and Docsify.txt' has 60 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Candidate Shortlisting Automation for ERPNext.txt'...\n",
            "  -> 'AI-Powered Candidate Shortlisting Automation for ERPNext.txt' has 39 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with local LLMs using n8n and Ollama.txt'...\n",
            "  -> 'Chat with local LLMs using n8n and Ollama.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Auto-Categorize blog posts in wordpress using A.I..txt'...\n",
            "  -> 'Auto-Categorize blog posts in wordpress using A.I..txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent for project management and meetings with Airtable and Fireflies.txt'...\n",
            "  -> 'AI Agent for project management and meetings with Airtable and Fireflies.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent for realtime insights on meetings.txt'...\n",
            "  -> 'AI Agent for realtime insights on meetings.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Ultimate Scraper Workflow for n8n.txt'...\n",
            "  -> 'Ultimate Scraper Workflow for n8n.txt' has 63 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Using External Workflows as Tools in n8n.txt'...\n",
            "  -> 'Using External Workflows as Tools in n8n.txt' has 4 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'CV Screening with OpenAI.txt'...\n",
            "  -> 'CV Screening with OpenAI.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate 9_16 Images from Content and Brand Guidelines.txt'...\n",
            "  -> 'Generate 9_16 Images from Content and Brand Guidelines.txt' has 39 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate SEO Seed Keywords Using AI.txt'...\n",
            "  -> 'Generate SEO Seed Keywords Using AI.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI.txt'...\n",
            "  -> 'Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI.txt' has 27 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Data Extraction with Dynamic Prompts and Baserow.txt'...\n",
            "  -> 'AI Data Extraction with Dynamic Prompts and Baserow.txt' has 45 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Integrating AI with Open-Meteo API for Enhanced Weather Forecasting.txt'...\n",
            "  -> 'Integrating AI with Open-Meteo API for Enhanced Weather Forecasting.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Auto Categorise Outlook Emails with AI.txt'...\n",
            "  -> 'Auto Categorise Outlook Emails with AI.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI agent chat.txt'...\n",
            "  -> 'AI agent chat.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Remove Personally Identifiable Information (PII) from CSV Files with OpenAI.txt'...\n",
            "  -> 'Remove Personally Identifiable Information (PII) from CSV Files with OpenAI.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Qualifying Appointment Requests with AI & n8n Forms.txt'...\n",
            "  -> 'Qualifying Appointment Requests with AI & n8n Forms.txt' has 25 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Upsert huge documents in a vector store with Supabase and Notion.txt'...\n",
            "  -> 'Upsert huge documents in a vector store with Supabase and Notion.txt' has 34 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate SQL queries from schema only - AI-powered.txt'...\n",
            "  -> 'Generate SQL queries from schema only - AI-powered.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Notion AI Assistant Generator.txt'...\n",
            "  -> 'Notion AI Assistant Generator.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Learn Anything from HN - Get Top Resource Recommendations from Hacker News.txt'...\n",
            "  -> 'Learn Anything from HN - Get Top Resource Recommendations from Hacker News.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyze & Sort Suspicious Email Contents with ChatGPT.txt'...\n",
            "  -> 'Analyze & Sort Suspicious Email Contents with ChatGPT.txt' has 25 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Visualize your SQL Agent queries with OpenAI and Quickchart.io.txt'...\n",
            "  -> 'Visualize your SQL Agent queries with OpenAI and Quickchart.io.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Customer Support Issue Resolution using AI Text Classifier.txt'...\n",
            "  -> 'Automate Customer Support Issue Resolution using AI Text Classifier.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Siri AI Agent_ Apple Shortcuts powered voice template.txt'...\n",
            "  -> 'Siri AI Agent_ Apple Shortcuts powered voice template.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Notion knowledge base AI assistant.txt'...\n",
            "  -> 'Notion knowledge base AI assistant.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Voice Chat using Webhook, Memory Manager, OpenAI, Google Gemini & ElevenLabs.txt'...\n",
            "  -> 'AI Voice Chat using Webhook, Memory Manager, OpenAI, Google Gemini & ElevenLabs.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI web researcher for sales.txt'...\n",
            "  -> 'AI web researcher for sales.txt' has 22 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Autonomous AI crawler.txt'...\n",
            "  -> 'Autonomous AI crawler.txt' has 38 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Transform Image to Lego Style Using Line and Dall-E.txt'...\n",
            "  -> 'Transform Image to Lego Style Using Line and Dall-E.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'LINE Assistant with Google Calendar and Gmail Integration.txt'...\n",
            "  -> 'LINE Assistant with Google Calendar and Gmail Integration.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Flux Dev Image Generation (Fal.ai) to Google Drive.txt'...\n",
            "  -> 'Flux Dev Image Generation (Fal.ai) to Google Drive.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent to chat with you Search Console Data, using OpenAI and Postgres.txt'...\n",
            "  -> 'AI Agent to chat with you Search Console Data, using OpenAI and Postgres.txt' has 30 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automatic Background Removal for Images in Google Drive.txt'...\n",
            "  -> 'Automatic Background Removal for Images in Google Drive.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Obsidian Notes Read Aloud using AI_ Available as a Podcast Feed.txt'...\n",
            "  -> 'Obsidian Notes Read Aloud using AI_ Available as a Podcast Feed.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Parse PDF with LlamaParse and save to Airtable.txt'...\n",
            "  -> Skipping file (parse error): Parse PDF with LlamaParse and save to Airtable.txt\n",
            "     Error: Extra data: line 601 column 2 (char 14316)\n",
            "Attempting to parse 'Screen Applicants With AI, notify HR and save them in a Google Sheet.txt'...\n",
            "  -> 'Screen Applicants With AI, notify HR and save them in a Google Sheet.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse '📈 Receive Daily Market News from FT.com to your Microsoft outlook inbox.txt'...\n",
            "  -> '📈 Receive Daily Market News from FT.com to your Microsoft outlook inbox.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'WordPress - AI Chatbot to enhance user experience - with Supabase and OpenAI.txt'...\n",
            "  -> 'WordPress - AI Chatbot to enhance user experience - with Supabase and OpenAI.txt' has 53 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Image Validation Tasks using AI Vision.txt'...\n",
            "  -> 'Automate Image Validation Tasks using AI Vision.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI agent that can scrape webpages.txt'...\n",
            "  -> 'AI agent that can scrape webpages.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Scrape and summarize webpages with AI.txt'...\n",
            "  -> 'Scrape and summarize webpages with AI.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyze Suspicious Email Contents with ChatGPT Vision.txt'...\n",
            "  -> 'Analyze Suspicious Email Contents with ChatGPT Vision.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Intelligent Web Query and Semantic Re-Ranking Flow using Brave and Google Gemini.txt'...\n",
            "  -> 'Intelligent Web Query and Semantic Re-Ranking Flow using Brave and Google Gemini.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract insights & analyse YouTube comments via AI Agent chat.txt'...\n",
            "  -> 'Extract insights & analyse YouTube comments via AI Agent chat.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Deduplicate Scraping AI Grants for Eligibility using AI.txt'...\n",
            "  -> 'Deduplicate Scraping AI Grants for Eligibility using AI.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Sales Meeting Prep with AI & APIFY Sent To WhatsApp.txt'...\n",
            "  -> 'Automate Sales Meeting Prep with AI & APIFY Sent To WhatsApp.txt' has 61 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Handling Job Application Submissions with AI and n8n Forms.txt'...\n",
            "  -> 'Handling Job Application Submissions with AI and n8n Forms.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Narrating over a Video using Multimodal AI.txt'...\n",
            "  -> 'Narrating over a Video using Multimodal AI.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram Bot with Supabase memory and OpenAI assistant integration.txt'...\n",
            "  -> 'Telegram Bot with Supabase memory and OpenAI assistant integration.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Daily Podcast Summary.txt'...\n",
            "  -> 'Daily Podcast Summary.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Transcribing Bank Statements To Markdown Using Gemini Vision AI.txt'...\n",
            "  -> 'Transcribing Bank Statements To Markdown Using Gemini Vision AI.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'CV Resume PDF Parsing with Multimodal Vision AI.txt'...\n",
            "  -> 'CV Resume PDF Parsing with Multimodal Vision AI.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Supabase Insertion & Upsertion & Retrieval.txt'...\n",
            "  -> 'Supabase Insertion & Upsertion & Retrieval.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram chat with PDF.txt'...\n",
            "  -> 'Telegram chat with PDF.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Advanced AI Demo (Presented at AI Developers #14 meetup).txt'...\n",
            "  -> 'Advanced AI Demo (Presented at AI Developers #14 meetup).txt' has 39 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Competitor Research with Exa.ai, Notion and AI Agents.txt'...\n",
            "  -> Skipping file (parse error): Automate Competitor Research with Exa.ai, Notion and AI Agents.txt\n",
            "     Error: Extra data: line 1359 column 2 (char 35062)\n",
            "Attempting to parse 'Actioning Your Meeting Next Steps using Transcripts and AI.txt'...\n",
            "  -> 'Actioning Your Meeting Next Steps using Transcripts and AI.txt' has 28 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Invoice data extraction with LlamaParse and OpenAI.txt'...\n",
            "  -> 'Invoice data extraction with LlamaParse and OpenAI.txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Gmail AI Auto-Responder_ Create Draft Replies to incoming emails.txt'...\n",
            "  -> 'Gmail AI Auto-Responder_ Create Draft Replies to incoming emails.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Write a WordPress post with AI (starting from a few keywords).txt'...\n",
            "  -> 'Write a WordPress post with AI (starting from a few keywords).txt' has 37 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with PDF docs using AI (quoting sources).txt'...\n",
            "  -> 'Chat with PDF docs using AI (quoting sources).txt' has 22 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automated Hugging Face Paper Summary Fetching & Categorization Workflow.txt'...\n",
            "  -> 'Automated Hugging Face Paper Summary Fetching & Categorization Workflow.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Vector Database as a Big Data Analysis Tool for AI Agents [3_3 - anomaly].txt'...\n",
            "  -> 'Vector Database as a Big Data Analysis Tool for AI Agents [3_3 - anomaly].txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Vector Database as a Big Data Analysis Tool for AI Agents [1_3 anomaly][1_2 KNN].txt'...\n",
            "  -> 'Vector Database as a Big Data Analysis Tool for AI Agents [1_3 anomaly][1_2 KNN].txt' has 25 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Prompt-based Object Detection with Gemini 2.0.txt'...\n",
            "  -> 'Prompt-based Object Detection with Gemini 2.0.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Dynamically generate a webpage from user request using OpenAI Structured Output.txt'...\n",
            "  -> 'Dynamically generate a webpage from user request using OpenAI Structured Output.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Invoice data extraction with LlamaParse and OpenAI (1).txt'...\n",
            "  -> 'Invoice data extraction with LlamaParse and OpenAI (1).txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with OpenAI Assistant (by adding a memory).txt'...\n",
            "  -> 'Chat with OpenAI Assistant (by adding a memory).txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Vector Database as a Big Data Analysis Tool for AI Agents [2_2 KNN].txt'...\n",
            "  -> 'Vector Database as a Big Data Analysis Tool for AI Agents [2_2 KNN].txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Vector Database as a Big Data Analysis Tool for AI Agents [2_3 - anomaly].txt'...\n",
            "  -> 'Vector Database as a Big Data Analysis Tool for AI Agents [2_3 - anomaly].txt' has 48 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Sentiment Analysis Tracking on Support Issues with Linear and Slack.txt'...\n",
            "  -> 'Sentiment Analysis Tracking on Support Issues with Linear and Slack.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Email Subscription Service with n8n Forms, Airtable and AI.txt'...\n",
            "  -> 'Email Subscription Service with n8n Forms, Airtable and AI.txt' has 32 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send Google analytics data to A.I. to analyze then save results in Baserow.txt'...\n",
            "  -> 'Send Google analytics data to A.I. to analyze then save results in Baserow.txt' has 22 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize your emails with A.I. (via Openrouter) and send to Line messenger.txt'...\n",
            "  -> 'Summarize your emails with A.I. (via Openrouter) and send to Line messenger.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Text automations using Apple Shortcuts.txt'...\n",
            "  -> 'Text automations using Apple Shortcuts.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'IT Ops AI SlackBot Workflow - Chat with your knowledge base.txt'...\n",
            "  -> 'IT Ops AI SlackBot Workflow - Chat with your knowledge base.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Dynamically generate a webpage from user request using OpenAI Structured Output (1).txt'...\n",
            "  -> 'Dynamically generate a webpage from user request using OpenAI Structured Output (1).txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with OpenAI Assistant (by adding a memory) (1).txt'...\n",
            "  -> 'Chat with OpenAI Assistant (by adding a memory) (1).txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Vector Database as a Big Data Analysis Tool for AI Agents [2_2 KNN] (1).txt'...\n",
            "  -> 'Vector Database as a Big Data Analysis Tool for AI Agents [2_2 KNN] (1).txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Sentiment Analysis Tracking on Support Issues with Linear and Slack (1).txt'...\n",
            "  -> 'Sentiment Analysis Tracking on Support Issues with Linear and Slack (1).txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Email Subscription Service with n8n Forms, Airtable and AI (1).txt'...\n",
            "  -> 'Email Subscription Service with n8n Forms, Airtable and AI (1).txt' has 32 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send Google analytics data to A.I. to analyze then save results in BaserowSend Google analytics data to A.I. to analyze then save results in Baserow.txt'...\n",
            "  -> 'Send Google analytics data to A.I. to analyze then save results in BaserowSend Google analytics data to A.I. to analyze then save results in Baserow.txt' has 22 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize your emails with A.I. (via Openrouter) and send to Line messenger (1).txt'...\n",
            "  -> 'Summarize your emails with A.I. (via Openrouter) and send to Line messenger (1).txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Text automations using Apple Shortcuts (1).txt'...\n",
            "  -> 'Text automations using Apple Shortcuts (1).txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enrich FAQ sections on your website pages at scale with AI.txt'...\n",
            "  -> 'Enrich FAQ sections on your website pages at scale with AI.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Easy Image Captioning with Gemini 1.5 Pro.txt'...\n",
            "  -> 'Easy Image Captioning with Gemini 1.5 Pro.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate audio from text using OpenAI and Webhook _ Text to Speech Workflow.txt'...\n",
            "  -> 'Generate audio from text using OpenAI and Webhook _ Text to Speech Workflow.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Turn Emails into AI-Enhanced Tasks in Notion (Multi-User Support) with Gmail, Airtable and Softr.txt'...\n",
            "  -> 'Turn Emails into AI-Enhanced Tasks in Notion (Multi-User Support) with Gmail, Airtable and Softr.txt' has 38 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Creating a AI Slack Bot with Google Gemini.txt'...\n",
            "  -> 'Creating a AI Slack Bot with Google Gemini.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Handling Appointment Leads and Follow-up With Twilio, Cal.com and AI.txt'...\n",
            "  -> 'Handling Appointment Leads and Follow-up With Twilio, Cal.com and AI.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Breakdown Documents into Study Notes using Templating MistralAI and Qdrant.txt'...\n",
            "  -> 'Breakdown Documents into Study Notes using Templating MistralAI and Qdrant.txt' has 42 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Build a Financial Documents Assistant using Qdrant and Mistral.ai.txt'...\n",
            "  -> 'Build a Financial Documents Assistant using Qdrant and Mistral.ai.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Talk to your SQLite database with a LangChain AI Agent.txt'...\n",
            "  -> 'Talk to your SQLite database with a LangChain AI Agent.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate LinkedIn Outreach with Notion and OpenAI.txt'...\n",
            "  -> 'Automate LinkedIn Outreach with Notion and OpenAI.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generate Text-to-Speech Using Elevenlabs via API.txt'...\n",
            "  -> 'Generate Text-to-Speech Using Elevenlabs via API.txt' has 6 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Auto-label incoming Gmail messages with AI nodes.txt'...\n",
            "  -> 'Auto-label incoming Gmail messages with AI nodes.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow.txt'...\n",
            "  -> 'AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with OpenAIs GPT via a simple Telegram Bot.txt'...\n",
            "  -> 'Chat with OpenAIs GPT via a simple Telegram Bot.txt' has 4 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with a Google Sheet using AI.txt'...\n",
            "  -> 'Chat with a Google Sheet using AI.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram AI bot with LangChain nodes.txt'...\n",
            "  -> 'Telegram AI bot with LangChain nodes.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI chat with any data source (using the n8n workflow tool).txt'...\n",
            "  -> 'AI chat with any data source (using the n8n workflow tool).txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Ask questions about a PDF using AI.txt'...\n",
            "  -> 'Ask questions about a PDF using AI.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI chatbot that can search the web.txt'...\n",
            "  -> 'AI chatbot that can search the web.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Suggest meeting slots using AI.txt'...\n",
            "  -> 'Suggest meeting slots using AI.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram AI Chatbot.txt'...\n",
            "  -> 'Telegram AI Chatbot.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract Information from a Logo Sheet using forms, AI, Google Sheet and Airtable.txt'...\n",
            "  -> 'Extract Information from a Logo Sheet using forms, AI, Google Sheet and Airtable.txt' has 44 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Get Airtable data via AI and Obsidian Notes.txt'...\n",
            "  -> 'Get Airtable data via AI and Obsidian Notes.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram AI Bot_ NeurochainAI Text & Image - NeurochainAI Basic API Integration.txt'...\n",
            "  -> 'Telegram AI Bot_ NeurochainAI Text & Image - NeurochainAI Basic API Integration.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Optimize & Update Printify Title and Description Workflow.txt'...\n",
            "  -> 'Optimize & Update Printify Title and Description Workflow.txt' has 26 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send daily translated Calvin and Hobbes Comics to Discord.txt'...\n",
            "  -> 'Send daily translated Calvin and Hobbes Comics to Discord.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'MongoDB AI Agent - Intelligent Movie Recommendations.txt'...\n",
            "  -> 'MongoDB AI Agent - Intelligent Movie Recommendations.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Telegram to Spotify with OpenAI.txt'...\n",
            "  -> 'Telegram to Spotify with OpenAI.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize Umami data with AI (via Openrouter) and save it to Baserow.txt'...\n",
            "  -> 'Summarize Umami data with AI (via Openrouter) and save it to Baserow.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Convert URL HTML to Markdown Format and Get Page Links.txt'...\n",
            "  -> 'Convert URL HTML to Markdown Format and Get Page Links.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🚀 Local Multi-LLM Testing & Performance Tracker.txt'...\n",
            "  -> '🚀 Local Multi-LLM Testing & Performance Tracker.txt' has 21 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'create e-mail responses with fastmail and OpenAI.txt'...\n",
            "  -> 'create e-mail responses with fastmail and OpenAI.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Manipulate PDF with Adobe developer API.txt'...\n",
            "  -> 'Manipulate PDF with Adobe developer API.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Visual Regression Testing with Apify and AI Vision Model.txt'...\n",
            "  -> 'Visual Regression Testing with Apify and AI Vision Model.txt' has 34 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract spending history from gmail to google sheet.txt'...\n",
            "  -> 'Extract spending history from gmail to google sheet.txt' has 24 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'KB Tool - Confluence Knowledge Base.txt'...\n",
            "  -> 'KB Tool - Confluence Knowledge Base.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Use AI to organize your Todoist Inbox.txt'...\n",
            "  -> 'Use AI to organize your Todoist Inbox.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Customer Insights with Qdrant, Python and Information Extractor.txt'...\n",
            "  -> 'Customer Insights with Qdrant, Python and Information Extractor.txt' has 37 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Survey Insights with Qdrant, Python and Information Extractor.txt'...\n",
            "  -> 'Survey Insights with Qdrant, Python and Information Extractor.txt' has 42 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Query n8n Credentials with AI SQL Agent.txt'...\n",
            "  -> 'Query n8n Credentials with AI SQL Agent.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enhance Customer Chat by Buffering Messages with Twilio and Redis.txt'...\n",
            "  -> 'Enhance Customer Chat by Buffering Messages with Twilio and Redis.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Generating Image Embeddings via Textual Summarisation.txt'...\n",
            "  -> 'Generating Image Embeddings via Textual Summarisation.txt' has 22 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Introduction to the HTTP Tool.txt'...\n",
            "  -> 'Introduction to the HTTP Tool.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Build a Tax Code Assistant with Qdrant, Mistral.ai and OpenAI.txt'...\n",
            "  -> 'Build a Tax Code Assistant with Qdrant, Mistral.ai and OpenAI.txt' has 38 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Reconcile Rent Payments with Local Excel Spreadsheet and OpenAI.txt'...\n",
            "  -> 'Reconcile Rent Payments with Local Excel Spreadsheet and OpenAI.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Organise Your Local File Directories With AI.txt'...\n",
            "  -> 'Organise Your Local File Directories With AI.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Recipe Recommendations with Qdrant and Mistral.txt'...\n",
            "  -> Skipping file (parse error): Recipe Recommendations with Qdrant and Mistral.txt\n",
            "     Error: Extra data: line 973 column 2 (char 21196)\n",
            "Attempting to parse 'Build Your Own Image Search Using AI Object Detection, CDN and ElasticSearchBuild Your Own Image Search Using AI Object Detection, CDN and ElasticSearch.txt'...\n",
            "  -> 'Build Your Own Image Search Using AI Object Detection, CDN and ElasticSearchBuild Your Own Image Search Using AI Object Detection, CDN and ElasticSearch.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enrich Property Inventory Survey with Image Recognition and AI Agent.txt'...\n",
            "  -> 'Enrich Property Inventory Survey with Image Recognition and AI Agent.txt' has 29 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Customer Support Channel and Ticketing System with Slack and Linear.txt'...\n",
            "  -> 'Customer Support Channel and Ticketing System with Slack and Linear.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Speed Up Social Media Banners With BannerBear.com.txt'...\n",
            "  -> 'Speed Up Social Media Banners With BannerBear.com.txt' has 16 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Your RFP Process with OpenAI Assistants.txt'...\n",
            "  -> 'Automate Your RFP Process with OpenAI Assistants.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Store Notion_s Pages as Vector Documents into Supabase with OpenAI.txt'...\n",
            "  -> 'Store Notion_s Pages as Vector Documents into Supabase with OpenAI.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Share YouTube Videos with AI Summaries on Discord.txt'...\n",
            "  -> 'Share YouTube Videos with AI Summaries on Discord.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Post New YouTube Videos to X.txt'...\n",
            "  -> 'Post New YouTube Videos to X.txt' has 6 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automated AI image analysis and response via Telegram.txt'...\n",
            "  -> 'Automated AI image analysis and response via Telegram.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Children_s English Storytelling on Telegram with OpenAI.txt'...\n",
            "  -> 'AI-Powered Children_s English Storytelling on Telegram with OpenAI.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Image Creation with OpenAI and Telegram.txt'...\n",
            "  -> 'Image Creation with OpenAI and Telegram.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Translate Telegram audio messages with AI (55 supported languages).txt'...\n",
            "  -> 'Translate Telegram audio messages with AI (55 supported languages).txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'OpenAI Assistant workflow_ upload file, create an Assistant, chat with it!.txt'...\n",
            "  -> 'OpenAI Assistant workflow_ upload file, create an Assistant, chat with it!.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Compose reply draft in Gmail with OpenAI Assistant.txt'...\n",
            "  -> 'Compose reply draft in Gmail with OpenAI Assistant.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Scrape and summarize posts of a news site without RSS feed using AI and save them to a NocoDB.txt'...\n",
            "  -> 'Scrape and summarize posts of a news site without RSS feed using AI and save them to a NocoDB.txt' has 36 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Transcribe Audio Files, Summarize with GPT-4, and Store in Notion.txt'...\n",
            "  -> 'Transcribe Audio Files, Summarize with GPT-4, and Store in Notion.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Extract data from resume and create PDF with Gotenberg.txt'...\n",
            "  -> 'Extract data from resume and create PDF with Gotenberg.txt' has 43 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'ChatGPT Automatic Code Review in Gitlab MR.txt'...\n",
            "  -> 'ChatGPT Automatic Code Review in Gitlab MR.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Qualify new leads in Google Sheets via OpenAI_s GPT-4.txt'...\n",
            "  -> 'Qualify new leads in Google Sheets via OpenAI_s GPT-4.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-powered WooCommerce Support-Agent.txt'...\n",
            "  -> 'AI-powered WooCommerce Support-Agent.txt' has 40 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Twitter Virtual AI Influencer.txt'...\n",
            "  -> 'Twitter Virtual AI Influencer.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Author and Publish Blog Posts From Google Sheets.txt'...\n",
            "  -> 'Author and Publish Blog Posts From Google Sheets.txt' has 35 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Bitrix24 Chatbot Application Workflow example with Webhook Integration.txt'...\n",
            "  -> 'Bitrix24 Chatbot Application Workflow example with Webhook Integration.txt' has 13 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Chat with your event schedule from Google Sheets in Telegram.txt'...\n",
            "  -> 'Chat with your event schedule from Google Sheets in Telegram.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Hacker News Job Listing Scraper and Parser.txt'...\n",
            "  -> 'Hacker News Job Listing Scraper and Parser.txt' has 20 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Agent with Ollama for current weather and wiki.txt'...\n",
            "  -> 'AI Agent with Ollama for current weather and wiki.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🤖🧑_💻 AI Agent for Top n8n Creators Leaderboard Reporting.txt'...\n",
            "  -> '🤖🧑_💻 AI Agent for Top n8n Creators Leaderboard Reporting.txt' has 49 node(s). Extracting unique signatures...\n",
            "Attempting to parse '🔥📈🤖 AI Agent for n8n Creators Leaderboard - Find Popular Workflows.txt'...\n",
            "  -> '🔥📈🤖 AI Agent for n8n Creators Leaderboard - Find Popular Workflows.txt' has 43 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize SERPBear data with AI (via Openrouter) and save it to Baserow.txt'...\n",
            "  -> 'Summarize SERPBear data with AI (via Openrouter) and save it to Baserow.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enhance Security Operations with the Qualys Slack Shortcut Bot!.txt'...\n",
            "  -> 'Enhance Security Operations with the Qualys Slack Shortcut Bot!.txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enhance Security Operations with the Qualys Slack Shortcut Bot! (1).txt'...\n",
            "  -> 'Enhance Security Operations with the Qualys Slack Shortcut Bot! (1).txt' has 23 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Monthly Spotify Track Archiving and Playlist Classification.txt'...\n",
            "  -> 'Monthly Spotify Track Archiving and Playlist Classification.txt' has 37 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Venafi Cloud Slack Cert Bot.txt'...\n",
            "  -> 'Venafi Cloud Slack Cert Bot.txt' has 38 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Enrich Pipedrive_s Organization Data with OpenAI GPT-4o & Notify it in Slack.txt'...\n",
            "  -> 'Enrich Pipedrive_s Organization Data with OpenAI GPT-4o & Notify it in Slack.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Classify lemlist replies using OpenAI and automate reply handling.txt'...\n",
            "  -> 'Classify lemlist replies using OpenAI and automate reply handling.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI-Powered Children_s Arabic Storytelling on Telegram.txt'...\n",
            "  -> 'AI-Powered Children_s Arabic Storytelling on Telegram.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Configure your own Image Creation API Using OpenAI DALLE-3.txt'...\n",
            "  -> 'Configure your own Image Creation API Using OpenAI DALLE-3.txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Automate Screenshots with URLbox & Analyze them with AI.txt'...\n",
            "  -> 'Automate Screenshots with URLbox & Analyze them with AI.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'vAssistant for Hubspot Chat using OpenAi and Airtable.txt'...\n",
            "  -> 'vAssistant for Hubspot Chat using OpenAi and Airtable.txt' has 34 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Summarize Google Sheets form feedback via OpenAI_s GPT-4.txt'...\n",
            "  -> 'Summarize Google Sheets form feedback via OpenAI_s GPT-4.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Classify new bugs in Linear with OpenAI_s GPT-4 and move them to the right team.txt'...\n",
            "  -> 'Classify new bugs in Linear with OpenAI_s GPT-4 and move them to the right team.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Ask a human for help when the AI doesn_t know the answer.txt'...\n",
            "  -> 'Ask a human for help when the AI doesn_t know the answer.txt' has 17 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Convert text to speech with OpenAI.txt'...\n",
            "  -> 'Convert text to speech with OpenAI.txt' has 8 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Translate audio using AI.txt'...\n",
            "  -> 'Translate audio using AI.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Qualify replies from Pipedrive persons with AI.txt'...\n",
            "  -> 'Qualify replies from Pipedrive persons with AI.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'OpenAI assistant with custom tools.txt'...\n",
            "  -> 'OpenAI assistant with custom tools.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI Customer feedback sentiment analysis.txt'...\n",
            "  -> 'AI Customer feedback sentiment analysis.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Prepare CSV files with GPT-4Prepare CSV files with GPT-4.txt'...\n",
            "  -> 'Prepare CSV files with GPT-4Prepare CSV files with GPT-4.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI_ Ask questions about any data source (using the n8n workflow retriever).txt'...\n",
            "  -> 'AI_ Ask questions about any data source (using the n8n workflow retriever).txt' has 7 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Force AI to use a specific output format.txt'...\n",
            "  -> 'Force AI to use a specific output format.txt' has 11 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'AI_ Summarize podcast episode and enhance using Wikipedia.txt'...\n",
            "  -> 'AI_ Summarize podcast episode and enhance using Wikipedia.txt' has 19 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Custom LangChain agent written in JavaScript.txt'...\n",
            "  -> 'Custom LangChain agent written in JavaScript.txt' has 10 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Discord AI-powered bot.txt'...\n",
            "  -> 'Discord AI-powered bot.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'OpenAI examples_ ChatGPT, DALLE-2, Whisper-1 – 5-in-1.txt'...\n",
            "  -> 'OpenAI examples_ ChatGPT, DALLE-2, Whisper-1 – 5-in-1.txt' has 27 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send a ChatGPT email reply and save responses to Google Sheets.txt'...\n",
            "  -> 'Send a ChatGPT email reply and save responses to Google Sheets.txt' has 49 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send specific PDF attachments from Gmail to Google Drive using OpenAI.txt'...\n",
            "  -> 'Send specific PDF attachments from Gmail to Google Drive using OpenAI.txt' has 18 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Reddit AI digest.txt'...\n",
            "  -> 'Reddit AI digest.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'lemlist __ GPT-3_ Supercharge your sales workflows.txt'...\n",
            "  -> Skipping file (parse error): lemlist __ GPT-3_ Supercharge your sales workflows.txt\n",
            "     Error: Extra data: line 397 column 2 (char 6577)\n",
            "Attempting to parse 'Automate testimonials in Strapi with n8n.txt'...\n",
            "  -> 'Automate testimonials in Strapi with n8n.txt' has 14 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'OpenAI-powered tweet generator.txt'...\n",
            "  -> 'OpenAI-powered tweet generator.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Send a random recipe once a day to Telegram.txt'...\n",
            "  -> 'Send a random recipe once a day to Telegram.txt' has 15 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Create dynamic Twitter profile banner.txt'...\n",
            "  -> 'Create dynamic Twitter profile banner.txt' has 12 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Update Twitter banner using HTTP request.txt'...\n",
            "  -> 'Update Twitter banner using HTTP request.txt' has 4 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Detect toxic language in Telegram messages.txt'...\n",
            "  -> Skipping file (parse error): Detect toxic language in Telegram messages.txt\n",
            "     Error: Extra data: line 148 column 2 (char 1960)\n",
            "Attempting to parse 'Add positive feedback messages to a table in Notion.txt'...\n",
            "  -> 'Add positive feedback messages to a table in Notion.txt' has 6 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'ETL pipeline for text processing.txt'...\n",
            "  -> 'ETL pipeline for text processing.txt' has 9 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyze feedback using AWS Comprehend and send it to a Mattermost channel.txt'...\n",
            "  -> 'Analyze feedback using AWS Comprehend and send it to a Mattermost channel.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Analyze feedback and send a message on Mattermost.txt'...\n",
            "  -> 'Analyze feedback and send a message on Mattermost.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'Create, update, and get a profile in Humantic AI.txt'...\n",
            "  -> 'Create, update, and get a profile in Humantic AI.txt' has 5 node(s). Extracting unique signatures...\n",
            "Attempting to parse 'ALL_unique_nodes.txt'...\n",
            "  -> Skipping file (parse error): ALL_unique_nodes.txt\n",
            "     Error: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Total unique node definitions found: 5434\n",
            "\n",
            "Wrote all unique nodes to:\n",
            " - /content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes.txt\n",
            " - /content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes.json\n",
            "=====================================================\n",
            "Processing complete!\n",
            " - Valid text/JSON files processed: 284\n",
            " - Unique node definitions found: 5434\n",
            " - Output folder: /content/drive/MyDrive/n8n_Workflow_Extracts\n",
            "=====================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have an LLM Make a Cheat Sheet"
      ],
      "metadata": {
        "id": "v3LZsh63UfU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "\n",
        "# For running in Colab (uncomment if not already done):\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ENTER_API_KEY\"\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()  # uses OPENAI_API_KEY from environment\n",
        "\n",
        "# Where the big JSON file from Phase 1 lives\n",
        "mega_json_path = \"/content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes.json\"\n",
        "\n",
        "# We'll write the summarized cheat sheet here\n",
        "cheatsheet_path = \"/content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes_cheatsheet.txt\"\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Read the big JSON\n",
        "# ---------------------------\n",
        "with open(mega_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    all_nodes = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(all_nodes)} total nodes from {mega_json_path}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Group by node[\"type\"]\n",
        "#     (Customize if you prefer resource-based grouping.)\n",
        "# ---------------------------\n",
        "from collections import defaultdict\n",
        "\n",
        "nodes_by_type = defaultdict(list)\n",
        "for nd in all_nodes:\n",
        "    ntype = nd.get(\"type\", \"unknown_type\")\n",
        "    nodes_by_type[ntype].append(nd)\n",
        "\n",
        "print(f\"Found {len(nodes_by_type)} unique node types.\\n\")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Summarize each group\n",
        "#   We'll do chunking if too large for a single call\n",
        "#   We'll then merge the partial summaries.\n",
        "# ---------------------------\n",
        "\n",
        "def summarize_nodes_with_o3mini(node_list):\n",
        "    \"\"\"\n",
        "    Summarize a subset of nodes with the o3-mini model.\n",
        "    Build a prompt that asks for a concise cheat sheet.\n",
        "    \"\"\"\n",
        "    # We'll convert them to JSON for context\n",
        "    # but keep it short if we can.\n",
        "    data_str = json.dumps(node_list, indent=2)\n",
        "\n",
        "    prompt_text = f\"\"\"\n",
        "You are an expert in n8n node JSON definitions.\n",
        "Below is a subset of nodes (in JSON).\n",
        "Please produce a short \"cheat sheet\" that explains:\n",
        "- The general structure for these node(s)\n",
        "- Key fields or parameters\n",
        "- Notable differences among them\n",
        "- A short example or best-practices tip.\n",
        "\n",
        "Nodes:\n",
        "{data_str}\n",
        "\n",
        "Cheat sheet:\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o3-mini\",\n",
        "        reasoning_effort=\"medium\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Helper to chunk a group of nodes so we don't exceed context limits\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Yield successive chunks of size chunk_size from lst.\"\"\"\n",
        "    for i in range(0, len(lst), chunk_size):\n",
        "        yield lst[i:i + chunk_size]\n",
        "\n",
        "# We'll define a rough chunk_size in # of nodes.\n",
        "# Adjust if you need smaller to avoid token overload.\n",
        "# If each node is large, reduce chunk_size.\n",
        "CHUNK_SIZE = 10\n",
        "\n",
        "all_summaries = []\n",
        "\n",
        "for ntype, nodelist in nodes_by_type.items():\n",
        "    print(f\"Summarizing node type: {ntype} (count={len(nodelist)})\")\n",
        "\n",
        "    # We might have to chunk if too many nodes\n",
        "    chunked_summaries = []\n",
        "    for chunk_idx, chunk in enumerate(chunk_list(nodelist, CHUNK_SIZE), start=1):\n",
        "        print(f\"  - Summarizing chunk #{chunk_idx} with {len(chunk)} nodes...\")\n",
        "        partial_summary = summarize_nodes_with_o3mini(chunk)\n",
        "        chunked_summaries.append(partial_summary)\n",
        "\n",
        "    # Combine the partial summaries for this type\n",
        "    combined_summary = f\"CHEAT SHEET FOR NODE TYPE: {ntype}\\n\\n\"\n",
        "    combined_summary += \"\\n\\n\".join(chunked_summaries)\n",
        "    combined_summary += \"\\n\\n\" + (\"=\"*60) + \"\\n\\n\"\n",
        "\n",
        "    all_summaries.append(combined_summary)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Write final cheat sheet\n",
        "# ---------------------------\n",
        "with open(cheatsheet_path, \"w\", encoding=\"utf-8\") as csf:\n",
        "    csf.write(\"\\n\".join(all_summaries))\n",
        "\n",
        "print(f\"\\nDone! Wrote cheat sheet to: {cheatsheet_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_2iEahSH1M1",
        "outputId": "3bfc1d12-679b-4c4b-eba2-c7360e9c87ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Loaded 5434 total nodes from /content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes.json.\n",
            "Found 195 unique node types.\n",
            "\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.chatTrigger (count=52)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.stickyNote (count=1529)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 10 nodes...\n",
            "  - Summarizing chunk #15 with 10 nodes...\n",
            "  - Summarizing chunk #16 with 10 nodes...\n",
            "  - Summarizing chunk #17 with 10 nodes...\n",
            "  - Summarizing chunk #18 with 10 nodes...\n",
            "  - Summarizing chunk #19 with 10 nodes...\n",
            "  - Summarizing chunk #20 with 10 nodes...\n",
            "  - Summarizing chunk #21 with 10 nodes...\n",
            "  - Summarizing chunk #22 with 10 nodes...\n",
            "  - Summarizing chunk #23 with 10 nodes...\n",
            "  - Summarizing chunk #24 with 10 nodes...\n",
            "  - Summarizing chunk #25 with 10 nodes...\n",
            "  - Summarizing chunk #26 with 10 nodes...\n",
            "  - Summarizing chunk #27 with 10 nodes...\n",
            "  - Summarizing chunk #28 with 10 nodes...\n",
            "  - Summarizing chunk #29 with 10 nodes...\n",
            "  - Summarizing chunk #30 with 10 nodes...\n",
            "  - Summarizing chunk #31 with 10 nodes...\n",
            "  - Summarizing chunk #32 with 10 nodes...\n",
            "  - Summarizing chunk #33 with 10 nodes...\n",
            "  - Summarizing chunk #34 with 10 nodes...\n",
            "  - Summarizing chunk #35 with 10 nodes...\n",
            "  - Summarizing chunk #36 with 10 nodes...\n",
            "  - Summarizing chunk #37 with 10 nodes...\n",
            "  - Summarizing chunk #38 with 10 nodes...\n",
            "  - Summarizing chunk #39 with 10 nodes...\n",
            "  - Summarizing chunk #40 with 10 nodes...\n",
            "  - Summarizing chunk #41 with 10 nodes...\n",
            "  - Summarizing chunk #42 with 10 nodes...\n",
            "  - Summarizing chunk #43 with 10 nodes...\n",
            "  - Summarizing chunk #44 with 10 nodes...\n",
            "  - Summarizing chunk #45 with 10 nodes...\n",
            "  - Summarizing chunk #46 with 10 nodes...\n",
            "  - Summarizing chunk #47 with 10 nodes...\n",
            "  - Summarizing chunk #48 with 10 nodes...\n",
            "  - Summarizing chunk #49 with 10 nodes...\n",
            "  - Summarizing chunk #50 with 10 nodes...\n",
            "  - Summarizing chunk #51 with 10 nodes...\n",
            "  - Summarizing chunk #52 with 10 nodes...\n",
            "  - Summarizing chunk #53 with 10 nodes...\n",
            "  - Summarizing chunk #54 with 10 nodes...\n",
            "  - Summarizing chunk #55 with 10 nodes...\n",
            "  - Summarizing chunk #56 with 10 nodes...\n",
            "  - Summarizing chunk #57 with 10 nodes...\n",
            "  - Summarizing chunk #58 with 10 nodes...\n",
            "  - Summarizing chunk #59 with 10 nodes...\n",
            "  - Summarizing chunk #60 with 10 nodes...\n",
            "  - Summarizing chunk #61 with 10 nodes...\n",
            "  - Summarizing chunk #62 with 10 nodes...\n",
            "  - Summarizing chunk #63 with 10 nodes...\n",
            "  - Summarizing chunk #64 with 10 nodes...\n",
            "  - Summarizing chunk #65 with 10 nodes...\n",
            "  - Summarizing chunk #66 with 10 nodes...\n",
            "  - Summarizing chunk #67 with 10 nodes...\n",
            "  - Summarizing chunk #68 with 10 nodes...\n",
            "  - Summarizing chunk #69 with 10 nodes...\n",
            "  - Summarizing chunk #70 with 10 nodes...\n",
            "  - Summarizing chunk #71 with 10 nodes...\n",
            "  - Summarizing chunk #72 with 10 nodes...\n",
            "  - Summarizing chunk #73 with 10 nodes...\n",
            "  - Summarizing chunk #74 with 10 nodes...\n",
            "  - Summarizing chunk #75 with 10 nodes...\n",
            "  - Summarizing chunk #76 with 10 nodes...\n",
            "  - Summarizing chunk #77 with 10 nodes...\n",
            "  - Summarizing chunk #78 with 10 nodes...\n",
            "  - Summarizing chunk #79 with 10 nodes...\n",
            "  - Summarizing chunk #80 with 10 nodes...\n",
            "  - Summarizing chunk #81 with 10 nodes...\n",
            "  - Summarizing chunk #82 with 10 nodes...\n",
            "  - Summarizing chunk #83 with 10 nodes...\n",
            "  - Summarizing chunk #84 with 10 nodes...\n",
            "  - Summarizing chunk #85 with 10 nodes...\n",
            "  - Summarizing chunk #86 with 10 nodes...\n",
            "  - Summarizing chunk #87 with 10 nodes...\n",
            "  - Summarizing chunk #88 with 10 nodes...\n",
            "  - Summarizing chunk #89 with 10 nodes...\n",
            "  - Summarizing chunk #90 with 10 nodes...\n",
            "  - Summarizing chunk #91 with 10 nodes...\n",
            "  - Summarizing chunk #92 with 10 nodes...\n",
            "  - Summarizing chunk #93 with 10 nodes...\n",
            "  - Summarizing chunk #94 with 10 nodes...\n",
            "  - Summarizing chunk #95 with 10 nodes...\n",
            "  - Summarizing chunk #96 with 10 nodes...\n",
            "  - Summarizing chunk #97 with 10 nodes...\n",
            "  - Summarizing chunk #98 with 10 nodes...\n",
            "  - Summarizing chunk #99 with 10 nodes...\n",
            "  - Summarizing chunk #100 with 10 nodes...\n",
            "  - Summarizing chunk #101 with 10 nodes...\n",
            "  - Summarizing chunk #102 with 10 nodes...\n",
            "  - Summarizing chunk #103 with 10 nodes...\n",
            "  - Summarizing chunk #104 with 10 nodes...\n",
            "  - Summarizing chunk #105 with 10 nodes...\n",
            "  - Summarizing chunk #106 with 10 nodes...\n",
            "  - Summarizing chunk #107 with 10 nodes...\n",
            "  - Summarizing chunk #108 with 10 nodes...\n",
            "  - Summarizing chunk #109 with 10 nodes...\n",
            "  - Summarizing chunk #110 with 10 nodes...\n",
            "  - Summarizing chunk #111 with 10 nodes...\n",
            "  - Summarizing chunk #112 with 10 nodes...\n",
            "  - Summarizing chunk #113 with 10 nodes...\n",
            "  - Summarizing chunk #114 with 10 nodes...\n",
            "  - Summarizing chunk #115 with 10 nodes...\n",
            "  - Summarizing chunk #116 with 10 nodes...\n",
            "  - Summarizing chunk #117 with 10 nodes...\n",
            "  - Summarizing chunk #118 with 10 nodes...\n",
            "  - Summarizing chunk #119 with 10 nodes...\n",
            "  - Summarizing chunk #120 with 10 nodes...\n",
            "  - Summarizing chunk #121 with 10 nodes...\n",
            "  - Summarizing chunk #122 with 10 nodes...\n",
            "  - Summarizing chunk #123 with 10 nodes...\n",
            "  - Summarizing chunk #124 with 10 nodes...\n",
            "  - Summarizing chunk #125 with 10 nodes...\n",
            "  - Summarizing chunk #126 with 10 nodes...\n",
            "  - Summarizing chunk #127 with 10 nodes...\n",
            "  - Summarizing chunk #128 with 10 nodes...\n",
            "  - Summarizing chunk #129 with 10 nodes...\n",
            "  - Summarizing chunk #130 with 10 nodes...\n",
            "  - Summarizing chunk #131 with 10 nodes...\n",
            "  - Summarizing chunk #132 with 10 nodes...\n",
            "  - Summarizing chunk #133 with 10 nodes...\n",
            "  - Summarizing chunk #134 with 10 nodes...\n",
            "  - Summarizing chunk #135 with 10 nodes...\n",
            "  - Summarizing chunk #136 with 10 nodes...\n",
            "  - Summarizing chunk #137 with 10 nodes...\n",
            "  - Summarizing chunk #138 with 10 nodes...\n",
            "  - Summarizing chunk #139 with 10 nodes...\n",
            "  - Summarizing chunk #140 with 10 nodes...\n",
            "  - Summarizing chunk #141 with 10 nodes...\n",
            "  - Summarizing chunk #142 with 10 nodes...\n",
            "  - Summarizing chunk #143 with 10 nodes...\n",
            "  - Summarizing chunk #144 with 10 nodes...\n",
            "  - Summarizing chunk #145 with 10 nodes...\n",
            "  - Summarizing chunk #146 with 10 nodes...\n",
            "  - Summarizing chunk #147 with 10 nodes...\n",
            "  - Summarizing chunk #148 with 10 nodes...\n",
            "  - Summarizing chunk #149 with 10 nodes...\n",
            "  - Summarizing chunk #150 with 10 nodes...\n",
            "  - Summarizing chunk #151 with 10 nodes...\n",
            "  - Summarizing chunk #152 with 10 nodes...\n",
            "  - Summarizing chunk #153 with 9 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatOpenAi (count=190)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 10 nodes...\n",
            "  - Summarizing chunk #15 with 10 nodes...\n",
            "  - Summarizing chunk #16 with 10 nodes...\n",
            "  - Summarizing chunk #17 with 10 nodes...\n",
            "  - Summarizing chunk #18 with 10 nodes...\n",
            "  - Summarizing chunk #19 with 10 nodes...\n",
            "Summarizing node type: n8n-nodes-base.set (count=446)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 10 nodes...\n",
            "  - Summarizing chunk #15 with 10 nodes...\n",
            "  - Summarizing chunk #16 with 10 nodes...\n",
            "  - Summarizing chunk #17 with 10 nodes...\n",
            "  - Summarizing chunk #18 with 10 nodes...\n",
            "  - Summarizing chunk #19 with 10 nodes...\n",
            "  - Summarizing chunk #20 with 10 nodes...\n",
            "  - Summarizing chunk #21 with 10 nodes...\n",
            "  - Summarizing chunk #22 with 10 nodes...\n",
            "  - Summarizing chunk #23 with 10 nodes...\n",
            "  - Summarizing chunk #24 with 10 nodes...\n",
            "  - Summarizing chunk #25 with 10 nodes...\n",
            "  - Summarizing chunk #26 with 10 nodes...\n",
            "  - Summarizing chunk #27 with 10 nodes...\n",
            "  - Summarizing chunk #28 with 10 nodes...\n",
            "  - Summarizing chunk #29 with 10 nodes...\n",
            "  - Summarizing chunk #30 with 10 nodes...\n",
            "  - Summarizing chunk #31 with 10 nodes...\n",
            "  - Summarizing chunk #32 with 10 nodes...\n",
            "  - Summarizing chunk #33 with 10 nodes...\n",
            "  - Summarizing chunk #34 with 10 nodes...\n",
            "  - Summarizing chunk #35 with 10 nodes...\n",
            "  - Summarizing chunk #36 with 10 nodes...\n",
            "  - Summarizing chunk #37 with 10 nodes...\n",
            "  - Summarizing chunk #38 with 10 nodes...\n",
            "  - Summarizing chunk #39 with 10 nodes...\n",
            "  - Summarizing chunk #40 with 10 nodes...\n",
            "  - Summarizing chunk #41 with 10 nodes...\n",
            "  - Summarizing chunk #42 with 10 nodes...\n",
            "  - Summarizing chunk #43 with 10 nodes...\n",
            "  - Summarizing chunk #44 with 10 nodes...\n",
            "  - Summarizing chunk #45 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.memoryBufferWindow (count=58)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleDocsTool (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleDocs (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.telegram (count=81)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.agent (count=106)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.aggregate (count=44)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.merge (count=94)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 4 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.chainLlm (count=71)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatOpenRouter (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.code (count=139)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 9 nodes...\n",
            "Summarizing node type: n8n-nodes-base.httpRequest (count=362)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 10 nodes...\n",
            "  - Summarizing chunk #15 with 10 nodes...\n",
            "  - Summarizing chunk #16 with 10 nodes...\n",
            "  - Summarizing chunk #17 with 10 nodes...\n",
            "  - Summarizing chunk #18 with 10 nodes...\n",
            "  - Summarizing chunk #19 with 10 nodes...\n",
            "  - Summarizing chunk #20 with 10 nodes...\n",
            "  - Summarizing chunk #21 with 10 nodes...\n",
            "  - Summarizing chunk #22 with 10 nodes...\n",
            "  - Summarizing chunk #23 with 10 nodes...\n",
            "  - Summarizing chunk #24 with 10 nodes...\n",
            "  - Summarizing chunk #25 with 10 nodes...\n",
            "  - Summarizing chunk #26 with 10 nodes...\n",
            "  - Summarizing chunk #27 with 10 nodes...\n",
            "  - Summarizing chunk #28 with 10 nodes...\n",
            "  - Summarizing chunk #29 with 10 nodes...\n",
            "  - Summarizing chunk #30 with 10 nodes...\n",
            "  - Summarizing chunk #31 with 10 nodes...\n",
            "  - Summarizing chunk #32 with 10 nodes...\n",
            "  - Summarizing chunk #33 with 10 nodes...\n",
            "  - Summarizing chunk #34 with 10 nodes...\n",
            "  - Summarizing chunk #35 with 10 nodes...\n",
            "  - Summarizing chunk #36 with 10 nodes...\n",
            "  - Summarizing chunk #37 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.splitInBatches (count=46)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolWikipedia (count=11)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.if (count=136)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 10 nodes...\n",
            "  - Summarizing chunk #11 with 10 nodes...\n",
            "  - Summarizing chunk #12 with 10 nodes...\n",
            "  - Summarizing chunk #13 with 10 nodes...\n",
            "  - Summarizing chunk #14 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.webhook (count=49)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 9 nodes...\n",
            "Summarizing node type: n8n-nodes-base.switch (count=56)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.postgresTool (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleDriveTrigger (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleDrive (count=50)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "Summarizing node type: n8n-nodes-base.errorTrigger (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.openAi (count=94)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.writeBinaryFile (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.readBinaryFile (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.formTrigger (count=18)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.extractFromFile (count=36)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.informationExtractor (count=25)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 5 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.chainSummarization (count=15)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleSheets (count=80)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.outputParserStructured (count=45)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 5 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolVectorStore (count=9)\n",
            "  - Summarizing chunk #1 with 9 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.vectorStoreQdrant (count=26)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.embeddingsOpenAi (count=46)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.manualTrigger (count=94)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 10 nodes...\n",
            "  - Summarizing chunk #10 with 4 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.documentDefaultDataLoader (count=33)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.textSplitterTokenSplitter (count=12)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.respondToWebhook (count=53)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.wordpress (count=13)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.vectorStorePinecone (count=16)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.embeddingsGoogleGemini (count=9)\n",
            "  - Summarizing chunk #1 with 9 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter (count=25)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.splitOut (count=85)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 10 nodes...\n",
            "  - Summarizing chunk #8 with 10 nodes...\n",
            "  - Summarizing chunk #9 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.emailReadImap (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.markdown (count=22)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.emailSend (count=16)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.gmail (count=43)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.textClassifier (count=15)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 5 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatDeepSeek (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.scheduleTrigger (count=42)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.postgres (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.facebookGraphApi (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.whatsApp (count=11)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolHttpRequest (count=23)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.executeWorkflowTrigger (count=38)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 8 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolWorkflow (count=40)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "Summarizing node type: n8n-nodes-base.microsoftOutlook (count=19)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 9 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatOllama (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.slack (count=23)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.convertToFile (count=14)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 4 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatGoogleGemini (count=33)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleSheetsTrigger (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmOllama (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.noOp (count=55)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.github (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.stopAndError (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-youtube-transcription.youtubeTranscripter (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.youTube (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.summarize (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.airtableTool (count=9)\n",
            "  - Summarizing chunk #1 with 9 nodes...\n",
            "Summarizing node type: n8n-nodes-base.airtable (count=65)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 10 nodes...\n",
            "  - Summarizing chunk #6 with 10 nodes...\n",
            "  - Summarizing chunk #7 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.limit (count=20)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "Summarizing node type: n8n-nodes-base.html (count=42)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.sentimentAnalysis (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.vectorStorePGVector (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.telegramTrigger (count=18)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 8 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.memoryPostgresChat (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleCalendarTool (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.form (count=13)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.rssFeedRead (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.outputParserAutofixing (count=11)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.gmailTrigger (count=16)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.zendesk (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolCalculator (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.wooCommerceTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.whatsAppTrigger (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.vectorStoreInMemory (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.supabase (count=12)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.vectorStoreSupabase (count=13)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.filter (count=42)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 10 nodes...\n",
            "  - Summarizing chunk #5 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.rssFeedReadTrigger (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.executeWorkflow (count=17)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.gmailTool (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.wait (count=31)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 10 nodes...\n",
            "  - Summarizing chunk #4 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.zoom (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.microsoftOutlookTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.clickUp (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.stravaTrigger (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.linkedIn (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.twitter (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleAnalyticsTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.bambooHr (count=5)\n",
            "  - Summarizing chunk #1 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.airtableTrigger (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.telegramTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.dateTime (count=5)\n",
            "  - Summarizing chunk #1 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mondayCom (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.baserowTool (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.executionData (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.removeDuplicates (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.quickChart (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.notion (count=13)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleAnalytics (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.hackerNews (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.s3 (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.dropbox (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.microsoftOneDrive (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.notionTrigger (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.memoryManager (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.crypto (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.redis (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatGroq (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mySqlTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.sort (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.readWriteFile (count=12)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.n8n (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.executeCommand (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.erpNext (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatAnthropic (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolSerpApi (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleCalendar (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.chainRetrievalQa (count=9)\n",
            "  - Summarizing chunk #1 with 9 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.retrieverVectorStore (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mySql (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.microsoftOutlookTrigger (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.jira (count=14)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.jiraTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.notionTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.editImage (count=18)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 8 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.manualChatTrigger (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.openAi (count=26)\n",
            "  - Summarizing chunk #1 with 10 nodes...\n",
            "  - Summarizing chunk #2 with 10 nodes...\n",
            "  - Summarizing chunk #3 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.compression (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmOpenAi (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleCloudStorage (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.openAiAssistant (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.graphql (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.baserow (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.strapi (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.webflow (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.twilioTrigger (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.twilio (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.localFileTrigger (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.embeddingsMistralCloud (count=5)\n",
            "  - Summarizing chunk #1 with 5 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.lmChatMistralCloud (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.outputParserItemList (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.documentBinaryInputLoader (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.itemLists (count=7)\n",
            "  - Summarizing chunk #1 with 7 nodes...\n",
            "Summarizing node type: n8n-nodes-base.discord (count=6)\n",
            "  - Summarizing chunk #1 with 6 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mongoDbTool (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.spotify (count=8)\n",
            "  - Summarizing chunk #1 with 8 nodes...\n",
            "Summarizing node type: n8n-nodes-base.linear (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.todoist (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.toolCode (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.elasticsearch (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.bannerbear (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.nocoDb (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.wooCommerce (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.dhl (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.function (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.venafiTlsProtectCloud (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.pipedriveTrigger (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.pipedrive (count=4)\n",
            "  - Summarizing chunk #1 with 4 nodes...\n",
            "Summarizing node type: n8n-nodes-base.lemlistTrigger (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.lemlist (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.linearTrigger (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.spreadsheetFile (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.moveBinaryData (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.retrieverWorkflow (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.documentJsonInputLoader (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: @n8n/n8n-nodes-langchain.code (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.readBinaryFiles (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.readPDF (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.reddit (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.interval (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.googleCloudNaturalLanguage (count=5)\n",
            "  - Summarizing chunk #1 with 5 nodes...\n",
            "Summarizing node type: n8n-nodes-base.functionItem (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.cron (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.start (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.typeformTrigger (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "Summarizing node type: n8n-nodes-base.trello (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mongoDb (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.mattermost (count=2)\n",
            "  - Summarizing chunk #1 with 2 nodes...\n",
            "Summarizing node type: n8n-nodes-base.awsComprehend (count=1)\n",
            "  - Summarizing chunk #1 with 1 nodes...\n",
            "Summarizing node type: n8n-nodes-base.humanticAi (count=3)\n",
            "  - Summarizing chunk #1 with 3 nodes...\n",
            "\n",
            "Done! Wrote cheat sheet to: /content/drive/MyDrive/n8n_Workflow_Extracts/ALL_unique_nodes_cheatsheet.txt\n"
          ]
        }
      ]
    }
  ]
}